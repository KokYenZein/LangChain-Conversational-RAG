{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 11, 'total_tokens': 20}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6e56c647-d89c-4d7a-9daa-950a99438b81-0', usage_metadata={'input_tokens': 11, 'output_tokens': 9, 'total_tokens': 20})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing for langSmith\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializeing model\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import bs4 # beautiful soup 4\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load, chunks and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Incorporate the retriever into a question-answering chain\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following piece of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable. Techniques like Chain of Thought and Tree of Thoughts help agents decompose hard tasks into multiple steps for better performance and interpretation of the model's thinking process. Task decomposition can be achieved through simple prompting, task-specific instructions, or human inputs.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have used the built-in chain constructors `create_stuff_documents_chain` and `create_retrieval_chain`, so that the basic ingredients to our solution are:\n",
    "1. retriever;\n",
    "2. prompt;\n",
    "3. LLM.\n",
    "  \n",
    "This will simplify the process of incorporating chat history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Chat History\n",
    "The chain we have built uses the input query directly to retrieve relevant context. But in a conversational setting, the user query might require conversational context to be understood.\n",
    "  \n",
    "  \n",
    "We'll need to update two things about our existing app:\n",
    "1. Prompt: Update our prompt to support historical messages as an input.\n",
    "2. Contextualizing questions: Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. This can be thought of simply as building a new \"history aware\" retriever. Whereas before we had:\n",
    "+ `query` -> `retriever`  \n",
    "Now we will have:\n",
    "+ `(query, conversation history)` -> `LLM` -> `rephrased query` -> `retriever`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable and easier to accomplish. This technique helps agents, such as autonomous systems, to plan ahead and execute tasks effectively by dividing them into achievable subgoals or components. Task decomposition can be facilitated by techniques like Chain of Thought and Tree of Thoughts, which guide the model to think step by step or explore multiple reasoning possibilities at each step.\n",
      "Common ways of extending the capabilities of LLMs through tool use include providing internet access for searches and information gathering, managing long-term memory effectively, delegating simple tasks to GPT-3.5 powered agents, and utilizing file output for storing and retrieving information. Performance evaluation methods involve continuously reviewing and analyzing actions, self-critiquing behavior, reflecting on past decisions, and aiming for smart and efficient task completion in minimal steps. Task decomposition can be achieved through simple prompting by LLMs, task-specific instructions, or human inputs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_1[\"answer\"])\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful Management of Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key = \"input\",\n",
    "    history_messages_key = \"chat_history\",\n",
    "    output_messages_key = \"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable and easier to solve. It can be achieved through techniques like Chain of Thought (CoT) or Tree of Thoughts, which prompt the model to think step by step or explore multiple reasoning possibilities at each step. Task decomposition can be done using simple prompting with LLMs, task-specific instructions, or human inputs to guide the agent through the process of breaking down the task into subgoals or smaller components.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    }, # constructs a key \"abc123\" in `store`\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Common ways of extending the capabilities of LLMs through tool use include providing internet access for searches and information gathering, managing long-term memory effectively, utilizing GPT-3.5 powered agents for delegating simple tasks, and enabling file output for storing information. These resources can enhance the performance of LLM-centered agents by expanding their access to external information, improving memory management, enabling task delegation, and facilitating data storage and retrieval.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}}\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable for an agent or system to handle. This approach helps in enhancing model performance on complex tasks by instructing the model to think step by step and decompose hard tasks into more manageable components. Techniques like Chain of Thought and Tree of Thoughts are used to transform big tasks into multiple smaller tasks, shedding light on the model's thinking process.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Common ways of extending the capabilities of LLMs through tool use include providing internet access for searches and information gathering, managing long-term memory effectively, utilizing GPT-3.5 powered agents for task delegation, and enabling file output for storing information. Performance evaluation techniques such as continuously reviewing actions, self-criticism, and reflecting on past decisions are also important in optimizing LLM performance. Task decomposition can be achieved through simple prompting by LLM, task-specific instructions, or human inputs to enhance the model's problem-solving abilities.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Common ways of extending the capabilities of LLMs through tool use include providing internet access for searches and information gathering, managing long-term memory effectively, utilizing GPT-3.5 powered agents for delegating simple tasks, and incorporating file output for storing and retrieving information. Performance evaluation is crucial, involving continuous review and analysis of actions, constructive self-criticism of behavior, reflection on past decisions, and aiming to complete tasks efficiently with minimal steps. Task decomposition can be achieved through simple prompting by the LLM, task-specific instructions, or human inputs to enhance the model's problem-solving abilities.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Common ways of extending model capabilities through tool use include equipping LLMs with external tools for tasks that exceed physical and cognitive limits, using task-specific instructions to guide the model in performing specific tasks like writing a novel, and incorporating human inputs for task decomposition. Additionally, employing prompts such as \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" can guide the LLM in utilizing external tools effectively.\n",
      "\n",
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition involves breaking down a complex task into smaller and simpler steps to make it more manageable and easier to solve. It can be achieved through techniques like Chain of Thought (CoT) or Tree of Thoughts, which prompt the model to think step by step or explore multiple reasoning possibilities at each step. Task decomposition can be done using simple prompting with LLMs, task-specific instructions, or human inputs to guide the agent through the process of breaking down the task into subgoals or smaller components.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Common ways of extending the capabilities of LLMs through tool use include providing internet access for searches and information gathering, managing long-term memory effectively, utilizing GPT-3.5 powered agents for delegating simple tasks, and enabling file output for storing information. These resources can enhance the performance of LLM-centered agents by expanding their access to external information, improving memory management, enabling task delegation, and facilitating data storage and retrieval.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The conversation history can be inspected in the store dict:\n",
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
